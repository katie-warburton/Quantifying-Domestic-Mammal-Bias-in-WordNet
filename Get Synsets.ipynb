{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Synsets\n",
    "\n",
    "Code to link Wikipedia pages for mammal species to WordNet synsets using the [BabelNet](https://babelnet.org/) API. This code helps with the analyses in Appendix D of \"Quantifying Bias in Hierarchical Category Systems\". BabelNet  v5.3 was accessed from November 26th - November 29th, 2023 for use in this paper.\n",
    "\n",
    "Mammal Wikipedia IDs (\"mammal_wiki_ids.txt\") were generated by scraping the Wikipedia page IDs for mammal species from various mammal lists on Wikipedia. The pages are listed below:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/List_of_placental_mammals\n",
    "- https://en.wikipedia.org/wiki/List_of_rodents\n",
    "- https://en.wikipedia.org/wiki/List_of_primates \n",
    "- https://en.wikipedia.org/wiki/List_of_lagomorphs \n",
    "- https://en.wikipedia.org/wiki/List_of_bats \n",
    "- https://en.wikipedia.org/wiki/List_of_carnivorans \n",
    "- https://en.wikipedia.org/wiki/List_of_artiodactyls \n",
    "- https://en.wikipedia.org/wiki/List_of_monotremes_and_marsupials\n",
    "\n",
    "Domestic Wikipedia IDs (\"domestic_wiki_ids.txt\") were generated by manually selecting the Wikipedia page IDs of mammals from the Domesticated Animals table on the following page: https://en.wikipedia.org/wiki/List_of_domesticated_animals. These Wikipedia pages are subject to change. All analyses used in the paper are based on when they were originally scraped on November 26th, 2023.\n",
    "\n",
    "\n",
    "\n",
    "Instructions for obtaining a BabelNet API key are found here: https://babelnet.org/guide.\n",
    "Due to API request limits, this code may have to be run in chunks over multiple days. The current place in the list of Wikipedia IDs is saved to \"current_idx.txt\" and parsed wordnet synsets are saved to either \"wild_nodes.txt\" or \"domestic_nodes.txt\".\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_IDS = []\n",
    "\n",
    "'''\n",
    "Find wikipedia pages in BabelNet and extract their corresponding\n",
    "WordNet synset if one exists.\n",
    "'''\n",
    "def get_synsets(wiki_ids, key):\n",
    "    not_in_wordnet = []\n",
    "    synsets = []\n",
    "    count = 0 # place in list\n",
    "    records_process = 0 # how many records were actually processed\n",
    "    flag = True\n",
    "    if key == '':\n",
    "        print('No BabelNet API key')\n",
    "        return None, count, False\n",
    "    for wiki_id in wiki_ids:\n",
    "        url1 = f'https://babelnet.io/v8/getSynsetIdsFromResourceID?id={wiki_id}&searchLang=EN&pos=NOUN&source=WIKI&key={key}'\n",
    "        response = requests.get(url1)\n",
    "        # If the API request limit is reached\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "            print(response.json())\n",
    "            flag = False\n",
    "            break\n",
    "        # If the API request limit is reached\n",
    "        babel = response.json()\n",
    "        if len(babel) == 0:\n",
    "            url1 = f'https://babelnet.io/v8/getSynsetIdsFromResourceID?id={wiki_id.lower()}&searchLang=EN&pos=NOUN&source=WIKI&key={key}'\n",
    "            response = requests.get(url1)\n",
    "            if response.status_code != 200:\n",
    "                print(response.status_code)\n",
    "                print(response.json())\n",
    "                flag = False\n",
    "                break\n",
    "            # If the wiki ID does not link to an entry in BabelNet\n",
    "            babel = response.json()\n",
    "            if len(babel) == 0:\n",
    "                print(f'ERROR: problem with wiki ID {wiki_id}')\n",
    "                BAD_IDS.append(wiki_id)\n",
    "                count += 1\n",
    "                continue\n",
    "        babel_id = babel[0]['id']\n",
    "        url2 = f'https://babelnet.io/v8/getSynset?id={babel_id}&key={key}'\n",
    "        response = requests.get(url2)\n",
    "        # If the API request limit is reached\n",
    "        if response.status_code != 200:\n",
    "            print(response.status_code)\n",
    "            print(response.json())\n",
    "            flag = False\n",
    "            break\n",
    "        senses = response.json()['senses']\n",
    "        synset_data = [data for data in senses if data['type'] == 'WordNetSense']\n",
    "        # If the BabelNet entry is not linked to a WordNet synset\n",
    "        if len(synset_data) == 0:\n",
    "            not_in_wordnet.append(wiki_id)\n",
    "        else: \n",
    "            # Extract WordNet synset\n",
    "            offset = synset_data[0]['properties']['wordNetOffset']\n",
    "            synsets.append(wn.of2ss(offset))\n",
    "        records_process += 1\n",
    "        count += 1\n",
    "        if records_process%10 == 2:\n",
    "            print(f'{records_process} records processed')\n",
    "    print(f'{records_process} records processed')\n",
    "    return synsets, count, flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5559 mammal wikipedia IDs and 28 domestic.\n"
     ]
    }
   ],
   "source": [
    "with open('mammal_wiki_ids.txt', 'r') as f:\n",
    "    mammalWiki = f.read().splitlines()\n",
    "\n",
    "with open('domestic_wiki_ids.txt', 'r') as f:\n",
    "    domesticWiki = f.read().splitlines()\n",
    "\n",
    "print(f'There are {len(mammalWiki)} mammal wikipedia IDs and {len(domesticWiki)} domestic.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Data\n",
    "Link wikipedia pages to wordnet sysnets using the BabelNet API. Variable \"key\" must be set to your BabelNet API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '' #BabelNet API Key\n",
    "try:\n",
    "    with open('current_idx.txt', 'r') as f:\n",
    "        idx = int(f.read().splitlines()[0])\n",
    "except FileNotFoundError:\n",
    "    idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = True\n",
    "while idx < len(mammalWiki) and flag:\n",
    "    print(f'Start Idx {idx}')\n",
    "    wild_nodes, count, flag = get_synsets(mammalWiki[idx:], key)\n",
    "    if wild_nodes is not None: \n",
    "        # Save mammal synsets found so far\n",
    "        with open('wild_nodes.txt', 'a') as f:\n",
    "            for node in wild_nodes:\n",
    "                if node is not None:\n",
    "                    f.write(f'{node.name()}\\n')\n",
    "    # If haven't finished crawling the entire list\n",
    "    if not flag: \n",
    "        idx += count\n",
    "        with open('current_idx.txt', 'w') as f:\n",
    "             f.write(f'{idx}')\n",
    "    print(f'End Idx {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domestic_nodes, count, flag = get_synsets(domesticWiki, key)\n",
    "if domestic_nodes is not None: \n",
    "    with open('domestic_nodes.txt', 'a') as f:\n",
    "        for node in wild_nodes:\n",
    "            if node is not None:\n",
    "                f.write(f'{node.name()}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimalityProj",
   "language": "python",
   "name": "optimalityproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
